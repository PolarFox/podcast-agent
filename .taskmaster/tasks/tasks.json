{
  "master": {
    "tasks": [
      {
        "id": 36,
        "title": "Create sources.yaml configuration file",
        "description": "Create the configuration file that defines RSS/HTTP sources and keywords for article collection",
        "details": "Create sources.yaml file with structure:\n```yaml\nsources:\n  - name: \"InfoQ\"\n    url: \"https://www.infoq.com/rss\"\n    category: \"architecture\"\n    authority_score: 0.9\n  - name: \"Martin Fowler\"\n    url: \"https://martinfowler.com/feed.atom\"\n    category: \"architecture\"\n    authority_score: 1.0\nkeywords:\n  agile: [\"scrum\", \"kanban\", \"sprint\", \"retrospective\"]\n  devops: [\"ci/cd\", \"kubernetes\", \"docker\", \"automation\"]\n  architecture: [\"microservices\", \"api\", \"design patterns\"]\n  leadership: [\"team management\", \"coaching\", \"culture\"]\n```\nInclude high-authority sources for each category (Agile, DevOps, Architecture/Infra, Leadership).",
        "testStrategy": "Validate YAML syntax and ensure all required fields are present. Test with sample RSS feeds to verify accessibility.",
        "priority": "high",
        "dependencies": [],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Define YAML schema and structure for sources configuration",
            "description": "Design the complete YAML schema structure for sources.yaml including all required fields, data types, and validation rules for RSS/HTTP sources and keyword definitions",
            "dependencies": [],
            "details": "Create the schema definition with sources array containing name, url, category, authority_score fields, and keywords object with category-specific keyword arrays. Define validation rules: authority_score must be 0.0-1.0, URLs must be valid HTTP/HTTPS, categories must be from predefined list (agile, devops, architecture, leadership). Document field requirements and optional parameters.",
            "status": "done",
            "testStrategy": "Create sample YAML files with valid and invalid data to test schema validation rules"
          },
          {
            "id": 2,
            "title": "Research and compile high-authority RSS sources for Agile category",
            "description": "Identify and validate RSS/Atom feeds from authoritative sources in the Agile methodology domain, including their URLs, reliability, and content quality",
            "dependencies": [
              "36.1"
            ],
            "details": "Research sources like Scrum.org, Agile Alliance, Mountain Goat Software, and other recognized Agile thought leaders. Verify RSS feed URLs are accessible and regularly updated. Assign appropriate authority scores based on source credibility and influence in the Agile community. Include at least 3-5 high-quality sources with scores 0.8-1.0.",
            "status": "pending",
            "testStrategy": "Test each RSS URL for accessibility and parse sample feeds to verify content quality and update frequency"
          },
          {
            "id": 3,
            "title": "Research and compile high-authority RSS sources for DevOps category",
            "description": "Identify and validate RSS/Atom feeds from authoritative sources in the DevOps domain, focusing on CI/CD, containerization, and automation topics",
            "dependencies": [
              "36.1"
            ],
            "details": "Research sources like Docker Blog, Kubernetes Blog, GitLab Blog, HashiCorp Blog, and other recognized DevOps platforms. Verify RSS feed URLs and content relevance to DevOps practices. Assign authority scores based on technical accuracy and industry influence. Include sources covering CI/CD, containerization, infrastructure as code, and monitoring.",
            "status": "pending",
            "testStrategy": "Validate RSS feeds and analyze recent articles to ensure content aligns with DevOps best practices"
          },
          {
            "id": 4,
            "title": "Research and compile high-authority RSS sources for Architecture and Leadership categories",
            "description": "Identify and validate RSS/Atom feeds for software architecture and technical leadership domains, including system design and team management sources",
            "dependencies": [
              "36.1"
            ],
            "details": "For Architecture: Research sources like Martin Fowler, InfoQ Architecture, High Scalability, and architecture-focused engineering blogs. For Leadership: Include sources like Rands in Repose, First Round Review, and technical leadership blogs. Verify feed accessibility and assign authority scores. Ensure coverage of microservices, system design, team management, and engineering culture topics.",
            "status": "pending",
            "testStrategy": "Test feed accessibility and review recent content to ensure alignment with architecture and leadership best practices"
          },
          {
            "id": 5,
            "title": "Create final sources.yaml file with comprehensive keyword definitions",
            "description": "Assemble the complete sources.yaml configuration file with all researched sources and comprehensive keyword mappings for each category",
            "dependencies": [
              "36.2",
              "36.3",
              "36.4"
            ],
            "details": "Combine all researched sources into the final YAML structure. Create comprehensive keyword arrays for each category: agile keywords (scrum, kanban, sprint, retrospective, user stories, velocity), devops keywords (ci/cd, kubernetes, docker, automation, infrastructure, monitoring), architecture keywords (microservices, api, design patterns, scalability, distributed systems), leadership keywords (team management, coaching, culture, mentoring, communication). Validate YAML syntax and ensure proper formatting.",
            "status": "pending",
            "testStrategy": "Validate YAML syntax using yaml.safe_load(), test file accessibility, and verify all RSS URLs are reachable with sample HTTP requests"
          }
        ]
      },
      {
        "id": 37,
        "title": "Implement RSS/HTTP article fetching module",
        "description": "Create Python module to fetch articles from sources defined in sources.yaml",
        "details": "Create fetch_articles.py module:\n```python\nimport feedparser\nimport requests\nimport yaml\nfrom datetime import datetime\nfrom dataclasses import dataclass\n\n@dataclass\nclass Article:\n    title: str\n    content: str\n    url: str\n    published: datetime\n    source: str\n    category: str\n\ndef load_sources(sources_file: str) -> dict:\n    with open(sources_file, 'r') as f:\n        return yaml.safe_load(f)\n\ndef fetch_rss_articles(source_config: dict) -> List[Article]:\n    feed = feedparser.parse(source_config['url'])\n    articles = []\n    for entry in feed.entries:\n        article = Article(\n            title=entry.title,\n            content=entry.summary,\n            url=entry.link,\n            published=datetime(*entry.published_parsed[:6]),\n            source=source_config['name'],\n            category=source_config['category']\n        )\n        articles.append(article)\n    return articles\n```\nHandle different RSS formats and HTTP errors gracefully.",
        "testStrategy": "Unit tests for each source type, mock RSS feeds for testing, verify article parsing accuracy and error handling for unreachable sources.",
        "priority": "high",
        "dependencies": [
          36
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement core Article dataclass and source configuration loading",
            "description": "Create the foundational Article dataclass and implement the load_sources function to read and parse the sources.yaml configuration file",
            "dependencies": [],
            "details": "Create fetch_articles.py with the Article dataclass containing title, content, url, published, source, and category fields. Implement load_sources function to read YAML configuration with proper error handling for file not found and invalid YAML format. Add validation for required fields in source configuration.",
            "status": "pending",
            "testStrategy": "Unit tests for Article dataclass instantiation, YAML loading with valid/invalid files, and source configuration validation"
          },
          {
            "id": 2,
            "title": "Implement RSS feed parsing with error handling",
            "description": "Create the fetch_rss_articles function to parse RSS feeds using feedparser with comprehensive error handling for different RSS formats and network issues",
            "dependencies": [
              "37.1"
            ],
            "details": "Implement fetch_rss_articles function using feedparser library. Handle various RSS/Atom formats by checking for different field names (published_parsed, updated_parsed, etc.). Add timeout configuration, retry logic for network failures, and graceful handling of malformed feeds. Include content extraction from both summary and content fields with fallback mechanisms.",
            "status": "pending",
            "testStrategy": "Mock RSS feeds with different formats (RSS 2.0, Atom), test network timeout scenarios, and verify parsing of various date formats"
          },
          {
            "id": 3,
            "title": "Implement HTTP article fetching for non-RSS sources",
            "description": "Add support for fetching articles directly from HTTP endpoints that don't provide RSS feeds, with content extraction and parsing",
            "dependencies": [
              "37.1"
            ],
            "details": "Extend the module with fetch_http_articles function for direct HTTP content fetching. Use requests library with proper headers, user-agent strings, and session management. Implement basic HTML parsing to extract article content using BeautifulSoup. Add support for different content types and encoding detection. Include rate limiting and respectful crawling practices.",
            "status": "pending",
            "testStrategy": "Test with various HTTP endpoints, different content encodings, and HTML structures. Mock HTTP responses for testing error conditions"
          },
          {
            "id": 4,
            "title": "Implement unified article fetching orchestrator",
            "description": "Create a main orchestrator function that coordinates fetching from all configured sources and handles different source types (RSS vs HTTP)",
            "dependencies": [
              "37.2",
              "37.3"
            ],
            "details": "Implement fetch_all_articles function that iterates through all sources from configuration, determines source type (RSS or HTTP), and calls appropriate fetching function. Add concurrent fetching using ThreadPoolExecutor for improved performance. Implement source-level error handling that doesn't stop processing of other sources. Include logging for successful and failed fetch operations.",
            "status": "pending",
            "testStrategy": "Integration tests with mixed source types, test concurrent fetching performance, and verify error isolation between sources"
          },
          {
            "id": 5,
            "title": "Add comprehensive error handling and logging",
            "description": "Implement robust error handling, logging, and monitoring for the entire article fetching pipeline",
            "dependencies": [
              "37.4"
            ],
            "details": "Add comprehensive logging using Python's logging module with different log levels for debugging, info, warnings, and errors. Implement custom exception classes for different error types (NetworkError, ParseError, ConfigError). Add retry mechanisms with exponential backoff for transient failures. Include metrics collection for successful/failed fetches per source. Add configuration options for timeout values, retry counts, and concurrent fetch limits.",
            "status": "pending",
            "testStrategy": "Test error scenarios including network failures, malformed feeds, invalid configurations, and verify proper logging output and retry behavior"
          }
        ]
      },
      {
        "id": 38,
        "title": "Implement article normalization and cleaning",
        "description": "Create module to normalize and clean article content for consistent processing",
        "details": "Create normalize.py module:\n```python\nimport re\nfrom bs4 import BeautifulSoup\nfrom typing import List\n\ndef normalize_article(article: Article) -> Article:\n    # Clean HTML tags\n    soup = BeautifulSoup(article.content, 'html.parser')\n    clean_content = soup.get_text()\n    \n    # Remove extra whitespace\n    clean_content = re.sub(r'\\s+', ' ', clean_content).strip()\n    \n    # Normalize title\n    clean_title = article.title.strip()\n    \n    # Create normalized article\n    return Article(\n        title=clean_title,\n        content=clean_content,\n        url=article.url,\n        published=article.published,\n        source=article.source,\n        category=article.category\n    )\n\ndef batch_normalize(articles: List[Article]) -> List[Article]:\n    return [normalize_article(article) for article in articles]\n```\nHandle various HTML formats and encoding issues.",
        "testStrategy": "Test with articles containing different HTML structures, special characters, and encoding. Verify content preservation while removing formatting.",
        "priority": "medium",
        "dependencies": [
          37
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create basic HTML cleaning and text extraction functionality",
            "description": "Implement core HTML parsing and cleaning functionality using BeautifulSoup to extract clean text content from various HTML formats",
            "dependencies": [],
            "details": "Create normalize.py with HTML cleaning functions: parse HTML using BeautifulSoup with 'html.parser', extract text content using get_text(), handle malformed HTML gracefully, preserve paragraph breaks where appropriate, and remove script/style tags completely. Include error handling for invalid HTML structures.",
            "status": "pending",
            "testStrategy": "Test with various HTML formats including nested tags, malformed HTML, empty content, and articles with embedded scripts/styles. Verify text extraction accuracy and error handling."
          },
          {
            "id": 2,
            "title": "Implement text normalization and whitespace cleaning",
            "description": "Create comprehensive text cleaning functions to normalize whitespace, handle special characters, and standardize text formatting",
            "dependencies": [
              "38.1"
            ],
            "details": "Implement regex-based text cleaning: remove extra whitespace using re.sub(r'\\s+', ' ', text), strip leading/trailing whitespace, normalize Unicode characters, handle special characters and encoding issues, preserve meaningful punctuation, and clean up common HTML entities that BeautifulSoup might miss.",
            "status": "pending",
            "testStrategy": "Test with articles containing multiple spaces, tabs, newlines, Unicode characters, and HTML entities. Verify proper whitespace normalization while preserving readability."
          },
          {
            "id": 3,
            "title": "Create Article object normalization function",
            "description": "Implement the main normalize_article function that processes individual Article objects and returns cleaned versions",
            "dependencies": [
              "38.1",
              "38.2"
            ],
            "details": "Create normalize_article function that takes an Article object, applies HTML cleaning to content, normalizes title by stripping whitespace, preserves metadata fields (url, published, source, category), handles None/empty values gracefully, and returns a new Article instance with cleaned data. Ensure immutability by creating new objects rather than modifying existing ones.",
            "status": "pending",
            "testStrategy": "Test with Article objects containing various content types, empty fields, None values, and different metadata combinations. Verify that original objects remain unchanged and new objects contain properly cleaned data."
          },
          {
            "id": 4,
            "title": "Implement batch processing functionality",
            "description": "Create batch_normalize function to efficiently process multiple articles and handle processing errors gracefully",
            "dependencies": [
              "38.3"
            ],
            "details": "Implement batch_normalize function that processes lists of Article objects, includes error handling for individual article failures, logs processing statistics, optionally supports parallel processing for large batches, and returns both successful and failed processing results. Include progress tracking for large batches.",
            "status": "pending",
            "testStrategy": "Test with various batch sizes including empty lists, single articles, and large collections. Verify error handling when individual articles fail processing and ensure batch processing doesn't stop on single failures."
          },
          {
            "id": 5,
            "title": "Add encoding detection and special character handling",
            "description": "Implement robust encoding detection and handling for various character encodings and international content",
            "dependencies": [
              "38.2"
            ],
            "details": "Add encoding detection using chardet library, handle UTF-8, Latin-1, and other common encodings, normalize Unicode characters to consistent forms, handle emoji and special symbols appropriately, detect and convert HTML entities, and provide fallback encoding strategies. Include configuration options for encoding preferences.",
            "status": "pending",
            "testStrategy": "Test with articles in different languages and encodings, content with emojis and special characters, and malformed encoding scenarios. Verify proper character preservation and encoding conversion accuracy."
          }
        ]
      },
      {
        "id": 39,
        "title": "Implement duplicate detection and removal",
        "description": "Create system to identify and remove duplicate articles using hash and similarity comparison",
        "details": "Create dedupe.py module:\n```python\nimport hashlib\nfrom difflib import SequenceMatcher\nfrom typing import List, Set\n\ndef calculate_content_hash(article: Article) -> str:\n    content = f\"{article.title}{article.content}\".lower()\n    return hashlib.md5(content.encode()).hexdigest()\n\ndef calculate_similarity(article1: Article, article2: Article) -> float:\n    text1 = f\"{article1.title} {article1.content}\".lower()\n    text2 = f\"{article2.title} {article2.content}\".lower()\n    return SequenceMatcher(None, text1, text2).ratio()\n\ndef remove_duplicates(articles: List[Article], similarity_threshold: float = 0.85) -> List[Article]:\n    unique_articles = []\n    seen_hashes = set()\n    \n    for article in articles:\n        content_hash = calculate_content_hash(article)\n        \n        # Check exact duplicates\n        if content_hash in seen_hashes:\n            continue\n            \n        # Check similarity with existing articles\n        is_duplicate = False\n        for existing in unique_articles:\n            if calculate_similarity(article, existing) > similarity_threshold:\n                is_duplicate = True\n                break\n                \n        if not is_duplicate:\n            unique_articles.append(article)\n            seen_hashes.add(content_hash)\n            \n    return unique_articles\n```",
        "testStrategy": "Test with known duplicate articles, near-duplicate content, and verify similarity threshold effectiveness. Benchmark performance with large article sets.",
        "priority": "medium",
        "dependencies": [
          38
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Article data model and base structure",
            "description": "Define the Article class/dataclass that will be used throughout the duplicate detection system, ensuring it has the necessary attributes for content hashing and similarity comparison",
            "dependencies": [],
            "details": "Create Article dataclass in models.py with attributes: title (str), content (str), url (str), published_date (datetime), source (str). Include __hash__ and __eq__ methods for proper comparison. Add validation for required fields and ensure content is properly encoded for hash calculations.",
            "status": "pending",
            "testStrategy": "Test Article creation with various content types, verify hash consistency, and test equality comparisons"
          },
          {
            "id": 2,
            "title": "Implement content hash calculation function",
            "description": "Create the calculate_content_hash function that generates MD5 hashes from article title and content for exact duplicate detection",
            "dependencies": [
              "39.1"
            ],
            "details": "Implement calculate_content_hash function in dedupe.py. Normalize text by converting to lowercase, removing extra whitespace, and handling special characters. Use MD5 hashing on UTF-8 encoded content. Include error handling for encoding issues and empty content scenarios.",
            "status": "pending",
            "testStrategy": "Test with identical articles producing same hash, different articles producing different hashes, and edge cases like empty content or special characters"
          },
          {
            "id": 3,
            "title": "Implement similarity calculation using SequenceMatcher",
            "description": "Create the calculate_similarity function that compares two articles using difflib.SequenceMatcher to detect near-duplicate content",
            "dependencies": [
              "39.1"
            ],
            "details": "Implement calculate_similarity function using SequenceMatcher.ratio() on combined title and content. Normalize text consistently with hash function. Add optional parameters for different similarity algorithms (title-only, content-only, weighted). Handle edge cases like empty articles or very short content.",
            "status": "pending",
            "testStrategy": "Test similarity scores with known similar/dissimilar article pairs, verify threshold effectiveness, and test performance with long articles"
          },
          {
            "id": 4,
            "title": "Create main duplicate removal algorithm",
            "description": "Implement the remove_duplicates function that combines hash-based exact matching with similarity-based near-duplicate detection",
            "dependencies": [
              "39.2",
              "39.3"
            ],
            "details": "Implement remove_duplicates function with configurable similarity threshold. Use set for O(1) hash lookups, then similarity comparison for remaining articles. Optimize by checking similarity only against recent unique articles (sliding window approach). Add logging for duplicate detection statistics and performance metrics.",
            "status": "pending",
            "testStrategy": "Test with datasets containing exact duplicates, near-duplicates, and unique articles. Verify performance with large article sets and different threshold values"
          },
          {
            "id": 5,
            "title": "Add configuration and optimization features",
            "description": "Enhance the duplicate detection system with configurable parameters, performance optimizations, and comprehensive error handling",
            "dependencies": [
              "39.4"
            ],
            "details": "Add DedupeConfig class for threshold settings, hash algorithms, and performance tuning. Implement batch processing for large datasets, memory-efficient processing with generators, and optional caching of similarity calculations. Add comprehensive logging, error handling, and metrics collection (duplicates found, processing time, memory usage).",
            "status": "pending",
            "testStrategy": "Test configuration flexibility, memory usage with large datasets, error recovery scenarios, and performance benchmarks with different optimization settings"
          }
        ]
      },
      {
        "id": 40,
        "title": "Implement AI-powered article classification and summarization",
        "description": "Create module to classify articles into categories and generate summaries using Ollama (dev) or Gemini Flash (prod)",
        "details": "Create ai_processor.py module:\n```python\nimport os\nimport requests\nfrom typing import Dict, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass ProcessedArticle:\n    original: Article\n    category: str\n    summary: str\n    team_impact: str\n    priority_score: float\n\nclass AIProcessor:\n    def __init__(self):\n        self.backend = os.getenv('AI_BACKEND', 'ollama')  # ollama or gemini\n        \n    def classify_and_summarize(self, article: Article) -> ProcessedArticle:\n        if self.backend == 'ollama':\n            return self._process_with_ollama(article)\n        else:\n            return self._process_with_gemini(article)\n    \n    def _process_with_ollama(self, article: Article) -> ProcessedArticle:\n        # Local Ollama API call\n        prompt = self._create_prompt(article)\n        response = requests.post('http://localhost:11434/api/generate', \n                               json={'model': 'llama2', 'prompt': prompt})\n        return self._parse_ai_response(article, response.json())\n    \n    def _process_with_gemini(self, article: Article) -> ProcessedArticle:\n        # Gemini Flash API call\n        api_key = os.getenv('GEMINI_API_KEY')\n        prompt = self._create_prompt(article)\n        # Implement Gemini API integration\n        pass\n    \n    def _create_prompt(self, article: Article) -> str:\n        return f\"\"\"Classify this article into one category (Agile, DevOps, Architecture, Leadership) and provide:\n1. Category\n2. TL;DR summary (2-3 sentences)\n3. Impact to development teams (bullet points)\n4. Priority score (0-1)\n\nArticle: {article.title}\n{article.content[:1000]}...\"\"\"\n```",
        "testStrategy": "Test both Ollama and Gemini backends with sample articles. Verify classification accuracy and summary quality. Test API error handling and fallback mechanisms.",
        "priority": "high",
        "dependencies": [
          39
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ProcessedArticle dataclass and AIProcessor base structure",
            "description": "Implement the core data structure and base AIProcessor class with initialization and backend selection logic",
            "dependencies": [],
            "details": "Create ai_processor.py with ProcessedArticle dataclass containing original, category, summary, team_impact, and priority_score fields. Implement AIProcessor.__init__ method with backend selection based on AI_BACKEND environment variable. Add classify_and_summarize method that routes to appropriate backend processor.",
            "status": "pending",
            "testStrategy": "Unit tests for dataclass instantiation, backend selection logic, and method routing based on environment variables"
          },
          {
            "id": 2,
            "title": "Implement prompt creation and response parsing utilities",
            "description": "Create shared utility methods for generating AI prompts and parsing structured responses from AI models",
            "dependencies": [
              "40.1"
            ],
            "details": "Implement _create_prompt method that formats article content into structured prompt requesting category classification, summary, team impact, and priority score. Create _parse_ai_response method to extract structured data from AI responses and construct ProcessedArticle objects. Handle various response formats and implement error handling for malformed responses.",
            "status": "pending",
            "testStrategy": "Test prompt generation with various article lengths and content types. Test response parsing with mock AI responses in different formats"
          },
          {
            "id": 3,
            "title": "Implement Ollama backend integration",
            "description": "Create the _process_with_ollama method to handle local Ollama API communication for development environment",
            "dependencies": [
              "40.2"
            ],
            "details": "Implement _process_with_ollama method using requests library to communicate with Ollama API at localhost:11434. Configure for llama2 model, handle API request/response cycle, implement timeout and retry logic. Add error handling for connection failures and invalid responses. Parse Ollama-specific response format.",
            "status": "pending",
            "testStrategy": "Integration tests with running Ollama instance. Mock tests for API failures and timeout scenarios. Verify response parsing accuracy"
          },
          {
            "id": 4,
            "title": "Implement Gemini Flash backend integration",
            "description": "Create the _process_with_gemini method to handle Google Gemini Flash API communication for production environment",
            "dependencies": [
              "40.2"
            ],
            "details": "Implement _process_with_gemini method using Google Gemini Flash API. Handle API key authentication from GEMINI_API_KEY environment variable. Implement proper request formatting for Gemini API, handle rate limiting and quota management. Add comprehensive error handling for authentication failures, API limits, and network issues.",
            "status": "pending",
            "testStrategy": "Integration tests with Gemini API using test credentials. Mock tests for authentication failures and rate limiting. Verify response format compatibility"
          },
          {
            "id": 5,
            "title": "Add error handling, fallback mechanisms, and configuration validation",
            "description": "Implement comprehensive error handling, backend fallback logic, and environment configuration validation",
            "dependencies": [
              "40.3",
              "40.4"
            ],
            "details": "Add try-catch blocks around all API calls with specific exception handling for network errors, authentication failures, and malformed responses. Implement fallback mechanism to switch between backends if primary fails. Add configuration validation to check required environment variables and API connectivity on initialization. Implement logging for debugging and monitoring.",
            "status": "pending",
            "testStrategy": "Test all error scenarios including network failures, invalid API keys, and malformed responses. Verify fallback mechanisms work correctly. Test configuration validation with missing environment variables"
          }
        ]
      },
      {
        "id": 41,
        "title": "Implement monthly data archive storage",
        "description": "Create persistent storage system for monthly article collections",
        "details": "Create archive.py module:\n```python\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import List\n\nclass MonthlyArchive:\n    def __init__(self, base_path: str = '/data/archive'):\n        self.base_path = Path(base_path)\n        self.base_path.mkdir(parents=True, exist_ok=True)\n    \n    def get_current_archive_path(self) -> Path:\n        current_month = datetime.now().strftime('%Y-%m')\n        return self.base_path / f'articles-{current_month}.json'\n    \n    def store_articles(self, articles: List[ProcessedArticle]):\n        archive_path = self.get_current_archive_path()\n        \n        # Load existing articles if file exists\n        existing_articles = []\n        if archive_path.exists():\n            with open(archive_path, 'r') as f:\n                existing_articles = json.load(f)\n        \n        # Convert articles to dict format\n        new_articles = [self._article_to_dict(article) for article in articles]\n        \n        # Merge and save\n        all_articles = existing_articles + new_articles\n        with open(archive_path, 'w') as f:\n            json.dump(all_articles, f, indent=2, default=str)\n    \n    def load_monthly_articles(self, year_month: str = None) -> List[dict]:\n        if not year_month:\n            year_month = datetime.now().strftime('%Y-%m')\n        \n        archive_path = self.base_path / f'articles-{year_month}.json'\n        if not archive_path.exists():\n            return []\n        \n        with open(archive_path, 'r') as f:\n            return json.load(f)\n```",
        "testStrategy": "Test file creation, article serialization/deserialization, and monthly archive rotation. Verify data persistence across application restarts.",
        "priority": "medium",
        "dependencies": [
          40
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create base MonthlyArchive class with directory structure",
            "description": "Implement the core MonthlyArchive class with initialization, directory creation, and path management functionality",
            "dependencies": [],
            "details": "Create archive.py module with MonthlyArchive class. Implement __init__ method to set up base_path using pathlib.Path, create directories with parents=True and exist_ok=True. Add get_current_archive_path method that generates monthly archive file paths using datetime.now().strftime('%Y-%m') format. Include proper error handling for directory creation failures.",
            "status": "pending",
            "testStrategy": "Test directory creation with various base paths, verify path generation for different months, test permission handling for directory creation"
          },
          {
            "id": 2,
            "title": "Implement article serialization helper methods",
            "description": "Create methods to convert ProcessedArticle objects to dictionary format for JSON storage and handle data type serialization",
            "dependencies": [
              "41.1"
            ],
            "details": "Add _article_to_dict method that converts ProcessedArticle objects to dictionary format suitable for JSON serialization. Handle datetime objects, nested data structures, and ensure all data types are JSON-serializable. Include proper handling of None values and edge cases. Add validation to ensure required fields are present before serialization.",
            "status": "pending",
            "testStrategy": "Test serialization with various ProcessedArticle objects, verify datetime handling, test with None values and edge cases"
          },
          {
            "id": 3,
            "title": "Implement store_articles method with merge functionality",
            "description": "Create the store_articles method that loads existing archives, merges new articles, and saves to monthly JSON files",
            "dependencies": [
              "41.2"
            ],
            "details": "Implement store_articles method that accepts List[ProcessedArticle]. Load existing articles from monthly archive file if it exists, handle file not found gracefully. Convert new articles using _article_to_dict, merge with existing articles, and save to JSON file with proper formatting (indent=2, default=str for datetime serialization). Include atomic write operations to prevent data corruption.",
            "status": "pending",
            "testStrategy": "Test with empty archives, existing archives, verify merge functionality, test atomic write operations and error recovery"
          },
          {
            "id": 4,
            "title": "Implement load_monthly_articles method with date handling",
            "description": "Create method to load articles from specific monthly archives with flexible date parameter handling",
            "dependencies": [
              "41.1"
            ],
            "details": "Implement load_monthly_articles method that accepts optional year_month parameter (format: 'YYYY-MM'). Default to current month if no parameter provided. Handle file existence checking, return empty list if archive doesn't exist. Include proper JSON loading with error handling for corrupted files. Add validation for year_month format.",
            "status": "pending",
            "testStrategy": "Test loading from existing and non-existing archives, verify date parameter handling, test with corrupted JSON files"
          },
          {
            "id": 5,
            "title": "Add archive management and utility methods",
            "description": "Implement additional utility methods for archive management including listing available archives and cleanup operations",
            "dependencies": [
              "41.3",
              "41.4"
            ],
            "details": "Add list_available_archives method that returns list of available monthly archives. Implement get_archive_stats method that returns statistics about archive sizes and article counts. Add cleanup_old_archives method with configurable retention period. Include proper error handling and logging for all operations. Add method to validate archive file integrity.",
            "status": "pending",
            "testStrategy": "Test archive listing functionality, verify statistics calculation, test cleanup operations with different retention periods, validate error handling"
          }
        ]
      },
      {
        "id": 42,
        "title": "Implement monthly prioritization and grouping system",
        "description": "Create system to prioritize, score, and group articles for monthly issue generation",
        "details": "Create prioritizer.py module:\n```python\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict\nfrom collections import defaultdict\nimport math\n\nclass ArticlePrioritizer:\n    def __init__(self, planning_horizon_weeks: int = 4):\n        self.planning_horizon = planning_horizon_weeks\n    \n    def calculate_priority_score(self, article: dict) -> float:\n        # Freshness decay (4 week horizon)\n        published = datetime.fromisoformat(article['published'])\n        age_days = (datetime.now() - published).days\n        freshness_score = math.exp(-age_days / 28)  # 4 week decay\n        \n        # Source authority (from sources.yaml)\n        authority_score = article.get('authority_score', 0.5)\n        \n        # AI-generated priority\n        ai_priority = article.get('priority_score', 0.5)\n        \n        # Category balance (prefer underrepresented categories)\n        category_balance = self._calculate_category_balance(article['category'])\n        \n        return (freshness_score * 0.3 + \n                authority_score * 0.3 + \n                ai_priority * 0.3 + \n                category_balance * 0.1)\n    \n    def group_related_articles(self, articles: List[dict]) -> List[List[dict]]:\n        # Group by similarity and topic\n        groups = []\n        ungrouped = articles.copy()\n        \n        while ungrouped:\n            seed_article = max(ungrouped, key=lambda x: x['priority_score'])\n            group = [seed_article]\n            ungrouped.remove(seed_article)\n            \n            # Find related articles\n            related = []\n            for article in ungrouped:\n                if self._are_related(seed_article, article):\n                    related.append(article)\n            \n            for article in related:\n                group.append(article)\n                ungrouped.remove(article)\n            \n            groups.append(group)\n        \n        return groups\n    \n    def _are_related(self, article1: dict, article2: dict) -> bool:\n        # Simple keyword overlap check\n        keywords1 = set(article1.get('keywords', []))\n        keywords2 = set(article2.get('keywords', []))\n        overlap = len(keywords1.intersection(keywords2))\n        return overlap >= 2 or article1['category'] == article2['category']\n```",
        "testStrategy": "Test scoring algorithm with various article types and ages. Verify grouping logic produces coherent topic clusters. Test category balance calculations.",
        "priority": "medium",
        "dependencies": [
          41
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement core priority scoring algorithm",
            "description": "Create the calculate_priority_score method that combines freshness decay, source authority, AI priority, and category balance into a single score",
            "dependencies": [],
            "details": "Implement the scoring algorithm with configurable weights: freshness_score (30%), authority_score (30%), ai_priority (30%), and category_balance (10%). Use exponential decay for freshness with 4-week horizon. Include validation for score ranges 0-1 and handle missing article fields gracefully.",
            "status": "pending",
            "testStrategy": "Test with articles of different ages, authority scores, and categories. Verify score ranges and weight distribution. Test edge cases with missing fields."
          },
          {
            "id": 2,
            "title": "Implement category balance calculation",
            "description": "Create the _calculate_category_balance method to prefer underrepresented categories in the current month's selection",
            "dependencies": [
              "42.1"
            ],
            "details": "Track category distribution in current selection and calculate balance scores to favor underrepresented categories. Maintain category counters and implement scoring that promotes diversity. Include configurable target distribution ratios for different categories.",
            "status": "pending",
            "testStrategy": "Test category distribution tracking with various article sets. Verify balance scoring promotes diversity and handles new categories appropriately."
          },
          {
            "id": 3,
            "title": "Implement article similarity detection",
            "description": "Create the _are_related method to identify related articles based on keyword overlap and category matching",
            "dependencies": [
              "42.1"
            ],
            "details": "Implement keyword intersection logic requiring minimum 2 keyword overlap or same category match. Add configurable similarity thresholds and support for semantic similarity if keywords are insufficient. Handle cases where articles lack keywords gracefully.",
            "status": "pending",
            "testStrategy": "Test with articles having various keyword overlaps and categories. Verify grouping accuracy and threshold effectiveness with sample article pairs."
          },
          {
            "id": 4,
            "title": "Implement article grouping algorithm",
            "description": "Create the group_related_articles method that clusters articles into coherent topic groups using similarity detection",
            "dependencies": [
              "42.3"
            ],
            "details": "Implement seed-based clustering starting with highest priority articles. Use similarity detection to find related articles and form groups. Ensure all articles are assigned to exactly one group and handle edge cases with isolated articles. Optimize for performance with large article sets.",
            "status": "pending",
            "testStrategy": "Test grouping with various article collections. Verify all articles are grouped, no duplicates exist across groups, and groups maintain topical coherence."
          },
          {
            "id": 5,
            "title": "Integrate and test complete prioritization system",
            "description": "Combine all components into the complete ArticlePrioritizer class and implement comprehensive testing",
            "dependencies": [
              "42.1",
              "42.2",
              "42.3",
              "42.4"
            ],
            "details": "Integrate all methods into the ArticlePrioritizer class with proper initialization and configuration. Add error handling, logging, and performance monitoring. Implement batch processing capabilities for large article collections and add configuration options for tuning algorithm parameters.",
            "status": "pending",
            "testStrategy": "End-to-end testing with realistic article datasets. Performance testing with large collections. Verify complete workflow from raw articles to prioritized groups produces expected results."
          }
        ]
      },
      {
        "id": 43,
        "title": "Implement GitHub Issues creation module",
        "description": "Create module to generate and create GitHub Issues from processed article groups",
        "details": "Create github_issues.py module:\n```python\nimport os\nimport requests\nfrom typing import List, Dict\nfrom datetime import datetime\n\nclass GitHubIssueCreator:\n    def __init__(self):\n        self.token = os.getenv('GITHUB_TOKEN')\n        self.repo_owner = os.getenv('GITHUB_REPO_OWNER')\n        self.repo_name = os.getenv('GITHUB_REPO_NAME')\n        self.base_url = f'https://api.github.com/repos/{self.repo_owner}/{self.repo_name}'\n    \n    def create_issue_from_group(self, article_group: List[dict]) -> dict:\n        # Generate issue content\n        title = self._generate_title(article_group)\n        body = self._generate_body(article_group)\n        labels = self._generate_labels(article_group)\n        \n        # Create issue via GitHub API\n        issue_data = {\n            'title': title,\n            'body': body,\n            'labels': labels\n        }\n        \n        response = requests.post(\n            f'{self.base_url}/issues',\n            headers={\n                'Authorization': f'token {self.token}',\n                'Accept': 'application/vnd.github.v3+json'\n            },\n            json=issue_data\n        )\n        \n        return response.json()\n    \n    def _generate_title(self, article_group: List[dict]) -> str:\n        primary_article = max(article_group, key=lambda x: x['priority_score'])\n        category_prefix = primary_article['category'].upper()[:4]\n        return f'[{category_prefix}] {primary_article[\"title\"][:50]}...'\n    \n    def _generate_body(self, article_group: List[dict]) -> str:\n        body_parts = []\n        \n        # Summary section\n        body_parts.append('## Summary')\n        for article in article_group:\n            body_parts.append(f'### {article[\"title\"]}')\n            body_parts.append(f'**Source:** {article[\"source\"]}')\n            body_parts.append(f'**TL;DR:** {article[\"summary\"]}')\n            body_parts.append(f'**Impact to teams:**')\n            body_parts.append(article['team_impact'])\n            body_parts.append(f'**Link:** {article[\"url\"]}')\n            body_parts.append('')\n        \n        # Recommendation\n        primary_category = article_group[0]['category']\n        recommendation_map = {\n            'Architecture': 'Architecture & Infra',\n            'DevOps': 'DevOps & Tools',\n            'Agile': 'Agile & Process',\n            'Leadership': 'Leadership & Culture'\n        }\n        \n        body_parts.append('## Recommendation')\n        body_parts.append(f'Sopii keskiviikon {recommendation_map.get(primary_category, \"General\")} -jaksoon')\n        \n        return '\\n'.join(body_parts)\n    \n    def _generate_labels(self, article_group: List[dict]) -> List[str]:\n        labels = ['draft']\n        categories = set(article['category'].lower() for article in article_group)\n        labels.extend(categories)\n        return labels\n```",
        "testStrategy": "Test GitHub API integration with mock data. Verify issue formatting and label assignment. Test authentication and error handling for API failures.",
        "priority": "high",
        "dependencies": [
          42
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement GitHubIssueCreator class initialization and configuration",
            "description": "Create the basic GitHubIssueCreator class with proper initialization, environment variable handling, and GitHub API configuration setup",
            "dependencies": [],
            "details": "Implement the __init__ method to load GitHub token, repository owner, and repository name from environment variables. Set up the base URL for GitHub API calls. Add validation to ensure all required environment variables are present and raise appropriate errors if missing. Include proper error handling for invalid repository configurations.",
            "status": "pending",
            "testStrategy": "Test with valid and invalid environment variables. Verify proper error messages for missing configurations. Mock environment variables to test different scenarios."
          },
          {
            "id": 2,
            "title": "Implement issue title generation logic",
            "description": "Create the _generate_title method to generate meaningful GitHub issue titles from article groups",
            "dependencies": [
              "43.1"
            ],
            "details": "Implement logic to identify the primary article with the highest priority score from the article group. Extract category prefix (first 4 characters, uppercase) and combine with truncated article title (50 characters max). Handle edge cases like empty titles, missing categories, or articles without priority scores. Ensure title format follows the pattern '[CATEGORY] Title...'",
            "status": "pending",
            "testStrategy": "Test with article groups containing different priority scores, categories, and title lengths. Verify proper truncation and category prefix formatting. Test edge cases with missing or invalid data."
          },
          {
            "id": 3,
            "title": "Implement issue body generation with structured content",
            "description": "Create the _generate_body method to format article group information into a structured GitHub issue body",
            "dependencies": [
              "43.1"
            ],
            "details": "Generate structured markdown content with Summary section containing article details (title, source, TL;DR, team impact, link) and Recommendation section with Finnish text mapping categories to meeting segments. Implement proper markdown formatting, handle multiple articles in a group, and ensure all article fields are properly displayed. Include error handling for missing article fields.",
            "status": "pending",
            "testStrategy": "Test with single and multiple article groups. Verify markdown formatting, Finnish recommendation text, and proper handling of missing article fields. Test with different category types."
          },
          {
            "id": 4,
            "title": "Implement label generation and GitHub API integration",
            "description": "Create the _generate_labels method and implement GitHub API communication for issue creation",
            "dependencies": [
              "43.1"
            ],
            "details": "Implement label generation logic that adds 'draft' label and category-based labels from article groups. Set up GitHub API POST request with proper authentication headers, request formatting, and response handling. Include comprehensive error handling for API failures, authentication issues, and network problems. Implement proper JSON serialization for issue data.",
            "status": "pending",
            "testStrategy": "Test label generation with various article categories. Mock GitHub API responses for success and error scenarios. Test authentication with valid and invalid tokens. Verify proper request formatting and error handling."
          },
          {
            "id": 5,
            "title": "Implement main issue creation workflow and error handling",
            "description": "Complete the create_issue_from_group method by integrating all components with comprehensive error handling and logging",
            "dependencies": [
              "43.2",
              "43.3",
              "43.4"
            ],
            "details": "Integrate title, body, and label generation methods into the main workflow. Add comprehensive error handling for API failures, network issues, and invalid article data. Implement proper logging for debugging and monitoring. Add response validation to ensure successful issue creation. Include retry logic for transient API failures and proper error reporting.",
            "status": "pending",
            "testStrategy": "Test end-to-end issue creation with valid article groups. Test error scenarios including API failures, network issues, and invalid data. Verify proper error messages and logging. Test with different article group sizes and structures."
          }
        ]
      },
      {
        "id": 44,
        "title": "Create Terraform configuration for Kubernetes deployment",
        "description": "Create Terraform configuration for CronJob deployment on K8s baremetal cluster",
        "details": "Create terraform/main.tf:\n```hcl\nvariable \"namespace\" {\n  description = \"Kubernetes namespace\"\n  type        = string\n  default     = \"podcast-agent\"\n}\n\nvariable \"image_pull_secret_name\" {\n  description = \"Name of existing imagePullSecret\"\n  type        = string\n}\n\nvariable \"app_env_configmap_name\" {\n  description = \"Name of existing ConfigMap with app environment\"\n  type        = string\n}\n\nvariable \"sources_configmap_name\" {\n  description = \"Name of existing ConfigMap with sources.yaml\"\n  type        = string\n}\n\nvariable \"github_secret_name\" {\n  description = \"Name of existing Secret with GitHub token\"\n  type        = string\n}\n\nresource \"kubernetes_namespace\" \"podcast_agent\" {\n  metadata {\n    name = var.namespace\n  }\n}\n\nresource \"kubernetes_service_account\" \"podcast_agent\" {\n  metadata {\n    name      = \"podcast-agent\"\n    namespace = var.namespace\n  }\n  \n  image_pull_secrets {\n    name = var.image_pull_secret_name\n  }\n}\n\nresource \"kubernetes_cron_job_v1\" \"article_collector\" {\n  metadata {\n    name      = \"article-collector\"\n    namespace = var.namespace\n  }\n  \n  spec {\n    schedule = \"0 6 * * *\"  # Daily at 06:00 UTC\n    \n    job_template {\n      metadata {}\n      \n      spec {\n        template {\n          metadata {}\n          \n          spec {\n            service_account_name = kubernetes_service_account.podcast_agent.metadata[0].name\n            restart_policy       = \"OnFailure\"\n            \n            container {\n              name  = \"podcast-agent\"\n              image = \"podcast-agent:latest\"\n              \n              env_from {\n                config_map_ref {\n                  name = var.app_env_configmap_name\n                }\n              }\n              \n              env_from {\n                secret_ref {\n                  name = var.github_secret_name\n                }\n              }\n              \n              volume_mount {\n                name       = \"sources-config\"\n                mount_path = \"/app/sources.yaml\"\n                sub_path   = \"sources.yaml\"\n              }\n              \n              volume_mount {\n                name       = \"data-archive\"\n                mount_path = \"/data/archive\"\n              }\n            }\n            \n            volume {\n              name = \"sources-config\"\n              config_map {\n                name = var.sources_configmap_name\n              }\n            }\n            \n            volume {\n              name = \"data-archive\"\n              persistent_volume_claim {\n                claim_name = kubernetes_persistent_volume_claim.archive_storage.metadata[0].name\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```",
        "testStrategy": "Validate Terraform syntax and plan execution. Test variable references and resource dependencies. Verify CronJob scheduling and container configuration.",
        "priority": "high",
        "dependencies": [
          43
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Terraform provider configuration and variables file",
            "description": "Set up the basic Terraform configuration with provider requirements and define all necessary input variables for the Kubernetes deployment",
            "dependencies": [],
            "details": "Create terraform/providers.tf with Kubernetes provider configuration and version constraints. Create terraform/variables.tf with all required variables including namespace, image_pull_secret_name, app_env_configmap_name, sources_configmap_name, and github_secret_name. Include proper descriptions, types, and default values where appropriate.",
            "status": "pending",
            "testStrategy": "Validate Terraform syntax with 'terraform validate' and verify provider initialization with 'terraform init'"
          },
          {
            "id": 2,
            "title": "Create persistent volume claim resource for data storage",
            "description": "Define the PersistentVolumeClaim resource that will be used by the CronJob for storing archived data",
            "dependencies": [
              "44.1"
            ],
            "details": "Add kubernetes_persistent_volume_claim resource named 'archive_storage' to main.tf. Configure appropriate storage class for bare metal cluster, set storage size (e.g., 10Gi), and access modes. Ensure the PVC is created in the correct namespace and has proper labels for identification.",
            "status": "pending",
            "testStrategy": "Verify PVC creation with 'terraform plan' and ensure storage class compatibility with bare metal cluster setup"
          },
          {
            "id": 3,
            "title": "Implement namespace and service account resources",
            "description": "Create the Kubernetes namespace and service account resources with proper image pull secrets configuration",
            "dependencies": [
              "44.1"
            ],
            "details": "Implement kubernetes_namespace resource for the podcast-agent namespace. Create kubernetes_service_account resource with reference to the image pull secret variable. Ensure proper metadata configuration and namespace association for the service account.",
            "status": "pending",
            "testStrategy": "Test namespace creation and service account configuration with proper image pull secret references"
          },
          {
            "id": 4,
            "title": "Configure CronJob container specification and volumes",
            "description": "Define the complete container specification for the CronJob including environment variables, volume mounts, and resource requirements",
            "dependencies": [
              "44.2",
              "44.3"
            ],
            "details": "Complete the kubernetes_cron_job_v1 resource with container specification including image reference, environment variable configuration from ConfigMaps and Secrets, volume mounts for sources config and data archive. Configure restart policy, service account reference, and proper volume definitions including ConfigMap and PVC volumes.",
            "status": "pending",
            "testStrategy": "Validate container configuration and volume mount paths. Test environment variable injection from ConfigMaps and Secrets"
          },
          {
            "id": 5,
            "title": "Add outputs and finalize Terraform configuration",
            "description": "Create outputs.tf file with useful output values and perform final validation of the complete Terraform configuration",
            "dependencies": [
              "44.4"
            ],
            "details": "Create terraform/outputs.tf with outputs for namespace name, service account name, CronJob name, and PVC name. Add any additional metadata outputs that might be useful for external references. Perform comprehensive validation of all resource dependencies and references within the Terraform configuration.",
            "status": "pending",
            "testStrategy": "Execute 'terraform validate' and 'terraform plan' to ensure all resources are properly configured and dependencies are correctly established. Verify output values are accessible and meaningful"
          }
        ]
      },
      {
        "id": 45,
        "title": "Create main application orchestrator and monthly analysis generator",
        "description": "Create main application that orchestrates daily collection and monthly issue generation, including situational analysis",
        "details": "Create main.py orchestrator:\n```python\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom fetch_articles import load_sources, fetch_rss_articles\nfrom normalize import batch_normalize\nfrom dedupe import remove_duplicates\nfrom ai_processor import AIProcessor\nfrom archive import MonthlyArchive\nfrom prioritizer import ArticlePrioritizer\nfrom github_issues import GitHubIssueCreator\n\nclass PodcastAgent:\n    def __init__(self):\n        self.ai_processor = AIProcessor()\n        self.archive = MonthlyArchive()\n        self.prioritizer = ArticlePrioritizer()\n        self.github = GitHubIssueCreator()\n    \n    def run_daily_collection(self):\n        \"\"\"Daily article collection and processing\"\"\"\n        print(f\"Starting daily collection at {datetime.now()}\")\n        \n        # Load sources and fetch articles\n        sources = load_sources('sources.yaml')\n        all_articles = []\n        \n        for source in sources['sources']:\n            articles = fetch_rss_articles(source)\n            all_articles.extend(articles)\n        \n        # Process articles\n        normalized = batch_normalize(all_articles)\n        deduplicated = remove_duplicates(normalized)\n        \n        # AI processing\n        processed_articles = []\n        for article in deduplicated:\n            processed = self.ai_processor.classify_and_summarize(article)\n            processed_articles.append(processed)\n        \n        # Store in monthly archive\n        self.archive.store_articles(processed_articles)\n        \n        print(f\"Processed {len(processed_articles)} articles\")\n    \n    def run_monthly_issue_generation(self, dry_run: bool = False):\n        \"\"\"Monthly issue generation and analysis\"\"\"\n        print(f\"Starting monthly issue generation at {datetime.now()}\")\n        \n        # Load monthly articles\n        articles = self.archive.load_monthly_articles()\n        \n        # Prioritize and group\n        for article in articles:\n            article['priority_score'] = self.prioritizer.calculate_priority_score(article)\n        \n        high_priority = [a for a in articles if a['priority_score'] > 0.7]\n        groups = self.prioritizer.group_related_articles(high_priority)\n        \n        # Remove duplicate groups\n        unique_groups = self._remove_duplicate_groups(groups)\n        \n        # Generate situational analysis\n        self._generate_situational_analysis(articles, unique_groups)\n        \n        # Create GitHub issues\n        if not dry_run:\n            for group in unique_groups:\n                issue = self.github.create_issue_from_group(group)\n                print(f\"Created issue: {issue['html_url']}\")\n        else:\n            print(f\"Dry run: Would create {len(unique_groups)} issues\")\n    \n    def _generate_situational_analysis(self, all_articles: list, selected_groups: list):\n        \"\"\"Generate monthly situational analysis\"\"\"\n        current_month = datetime.now().strftime('%Y-%m')\n        analysis_path = Path(f'docs/analysis/situational-{current_month}.md')\n        analysis_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Generate analysis content\n        analysis_content = self._create_analysis_content(all_articles, selected_groups)\n        \n        with open(analysis_path, 'w') as f:\n            f.write(analysis_content)\n        \n        print(f\"Generated situational analysis: {analysis_path}\")\n\nif __name__ == '__main__':\n    agent = PodcastAgent()\n    \n    mode = os.getenv('RUN_MODE', 'daily')\n    if mode == 'daily':\n        agent.run_daily_collection()\n    elif mode == 'monthly':\n        dry_run = os.getenv('DRY_RUN', 'false').lower() == 'true'\n        agent.run_monthly_issue_generation(dry_run=dry_run)\n```",
        "testStrategy": "Test both daily and monthly modes with sample data. Verify error handling and logging. Test dry-run functionality and analysis generation.",
        "priority": "medium",
        "dependencies": [
          44
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create PodcastAgent class initialization and configuration",
            "description": "Implement the main PodcastAgent class with proper initialization of all required components including AI processor, archive, prioritizer, and GitHub issue creator",
            "dependencies": [],
            "details": "Create the PodcastAgent class constructor that initializes AIProcessor, MonthlyArchive, ArticlePrioritizer, and GitHubIssueCreator instances. Add error handling for component initialization failures and configuration validation. Include logging setup for the orchestrator.",
            "status": "pending",
            "testStrategy": "Unit test class initialization with mocked dependencies. Verify all components are properly instantiated and configuration is loaded correctly."
          },
          {
            "id": 2,
            "title": "Implement daily article collection workflow",
            "description": "Create the run_daily_collection method that orchestrates the complete daily workflow from fetching articles to storing processed results",
            "dependencies": [
              "45.1"
            ],
            "details": "Implement the daily collection workflow that loads sources from sources.yaml, fetches articles from all RSS feeds, normalizes and deduplicates them, processes through AI classification and summarization, and stores results in monthly archive. Include progress logging and error handling for each step.",
            "status": "pending",
            "testStrategy": "Integration test with sample RSS feeds and mock AI processing. Verify article flow through all processing stages and proper storage in archive."
          },
          {
            "id": 3,
            "title": "Implement monthly issue generation workflow",
            "description": "Create the run_monthly_issue_generation method that loads monthly articles, prioritizes them, groups related content, and creates GitHub issues",
            "dependencies": [
              "45.1"
            ],
            "details": "Implement monthly workflow that loads articles from archive, calculates priority scores, filters high-priority articles, groups related content, removes duplicate groups, and creates GitHub issues. Include dry-run functionality and comprehensive logging of the selection process.",
            "status": "pending",
            "testStrategy": "Test with sample monthly data including priority calculation, grouping logic, and GitHub issue creation. Verify dry-run mode works correctly without creating actual issues."
          },
          {
            "id": 4,
            "title": "Implement situational analysis generation",
            "description": "Create the _generate_situational_analysis method and supporting _create_analysis_content method to generate monthly markdown reports",
            "dependencies": [
              "45.3"
            ],
            "details": "Implement analysis generation that creates comprehensive monthly situational reports in markdown format. Include statistics on article volumes, trending topics, priority distributions, and selected issue groups. Generate structured analysis with sections for overview, key trends, selected topics, and recommendations.",
            "status": "pending",
            "testStrategy": "Test analysis generation with various monthly datasets. Verify markdown formatting, content accuracy, and file creation in correct directory structure."
          },
          {
            "id": 5,
            "title": "Implement main execution logic and environment handling",
            "description": "Create the main execution block that handles different run modes (daily/monthly) based on environment variables and command-line arguments",
            "dependencies": [
              "45.2",
              "45.3",
              "45.4"
            ],
            "details": "Implement the main execution logic that reads RUN_MODE environment variable to determine whether to run daily collection or monthly issue generation. Handle DRY_RUN environment variable for testing. Add proper error handling, logging configuration, and graceful shutdown. Include command-line argument parsing for manual execution.",
            "status": "pending",
            "testStrategy": "Test both daily and monthly execution modes with different environment variable combinations. Verify error handling for invalid configurations and proper logging output."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-21T08:00:58.083Z",
      "updated": "2025-09-22T05:36:30.675Z",
      "description": "Tasks for master context"
    }
  }
}